[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT765 Assignment",
    "section": "",
    "text": "1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#objective",
    "href": "index.html#objective",
    "title": "STAT765 Assignment",
    "section": "1.1 Objective",
    "text": "1.1 Objective\nThe objective of this project is to produce 3-day forecasts of net energy use for the Cohaus residential development of 20 apartments at 11-13 Surrey Crescent, Grey Lynn, Auckland.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "STAT765 Assignment",
    "section": "1.2 Data",
    "text": "1.2 Data\nThe data for this project will be collected from the residential development itself (of which the author is an owner-resident). In network terms, Cohaus is a “customer network” meaning that its Body Corporate negotiate one whoesale energy supply contract with an electricity retailer and then individually meters each of the 20 dwellings based on their energy consumption. Cohaus has also installed solar generation capacity that is applied locally, and if any surplus remains is sold back to the electricity retailer.\nI have nearly 3 years of aggregate and anonymized individual 30-minute metered electricity usage data from the 20 private dwellings and will also include metered usage in the commons (including centralised hot water system, electric car chargers, communal laundry, bike shed and garden house). Solar power generation data is metered on 30-minute intervals as well.\nExogenous variables, especially weather data (ambient and dry temperatures, sunlight hours, cloud cover, precipitation) will also be collected from NIWA and incorporated into the model. As energy consumption exhibits significant seasonality (intra-day, daily, weekly, monthly) these factors will need to be incorporated into the model.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#exploratory-ideas",
    "href": "index.html#exploratory-ideas",
    "title": "STAT765 Assignment",
    "section": "1.3 Exploratory Ideas",
    "text": "1.3 Exploratory Ideas\nTo start with, the data on solar generation, centralised hot water, and electric car pool usage will be collected and organized. The data will be cleaned and processed to remove any missing or erroneous entries. Next, time series analysis techniques will be applied to the solar generation data to forecast future energy production. Statistical models will be developed to forecast the demand for centralised hot water based on historical consumption patterns. The forecasts for private and communal energy consumption and solar generation will be netted to provide a comprehensive short term consumption / generation forecast for Cohaus.\nAfter wrangling the data, I’m key to do a “shoot out” of sorts by training a battery (no pun intended) of short-term prediction models on the data given. From simple to more complex I hope to build the following models:\n\nNaive model with seasonal adjustments (intra-day, intra-week, inter-quarter)\nSTL1 model\nARIMAx2 model\nProphet model\nRecurring Neural Network (possibly Long-Short Term Memory) model",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#challenges",
    "href": "index.html#challenges",
    "title": "STAT765 Assignment",
    "section": "1.4 Challenges",
    "text": "1.4 Challenges\nThe accuracy of the short term forecasts may be affected by external factors such as weather conditions, changes in occupancy patterns, and variations in electric car usage. These factors will need to be carefully considered and accounted for in the forecasting models. The availability and quality of data on solar generation, centralised hot water, and electric car usage may vary. The first year of data collected includes considerable time when residents were in COVID-19 related lockdowns, and this may not prove valuable in training a model to produce short term forecasts now. Seasonality is layered and complex (spanning intra-day through inter-quarter) and will need to be modeled carefully. Data collection and data quality assurance processes will need to be implemented to ensure reliable and accurate forecasts. The integration of forecasts for energy generation, hot water demand, and electric car usage may require the development of a sophisticated modeling framework that can capture the interdependencies between these variables.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "STAT765 Assignment",
    "section": "",
    "text": "Seasonal and Trend decomposition using Loess.↩︎\nAuto Regressive Integrated Moving Average with exogenous variables.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "2  Data Processing",
    "section": "",
    "text": "2.1 Goal\nThe objective of this project is to produce 3-day forecasts of net energy use for the Cohaus residential development of 20 apartments at 11-13 Surrey Crescent, Grey Lynn, Auckland.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Processing</span>"
    ]
  },
  {
    "objectID": "data.html#data-source",
    "href": "data.html#data-source",
    "title": "2  Data Processing",
    "section": "2.2 Data Source",
    "text": "2.2 Data Source\nCohaus was completed in July 2021 and teh community moved in shortly thereafter,, Cohaus has installed solar generation capacity that is applied locally, and if any surplus remains is sold back to the electricity retailer. The data for this project has been collected from the residential development itself (of which the author is an owner-resident). Hourly kWh electricity use and generation measures for the period from January 1, 2022 0:00 NZST through February 29, 2024 23:00 NZST inclusive are categorised follows:\n\nAPT aggregate apartment electricity use\nCS common services (lighting, laundry, bike shed, garden house)\nEV electric vehicle chargers\nHP centralised hot water and heat pumps\nGen photo-voltaic electricity generation\n\nNet energy use NetUse (i.e. pulled / (pushed) to the grid) is calculated by summing these five measures. Note in my wrangling, I have down-sampled to an hourly resolution.\nFor each hourly period, if NetUse is positive, then Cohaus is importing electricity from the grid. If NetUse is negative, then Cohaus is exporting electricity to the grid. The difference between Import and Export is the NetImport. It is important to note that the Use/Gen and Import/Export measures are different for each time period due to the offset of use and generation within each period. This reflects the fact that depending on the hour of the day, Cohaus’ solar generation may be in deficit or surplus to the demand of the residents and there is no battery storage onsite, necessitating an import or export from the grid.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Processing</span>"
    ]
  },
  {
    "objectID": "data.html#meteorological-data",
    "href": "data.html#meteorological-data",
    "title": "2  Data Processing",
    "section": "2.3 Meteorological Data",
    "text": "2.3 Meteorological Data\nElectricity Use and Generation at Cohaus is influenced by a number of meteorological factors. For example, heating when temperatures are below 20\\(^{\\circ}\\)C. I collected hourly meteorological data from CliFlo for the weather station at MOTAT (Auckland Museum of Transport and Technology) which is the closest to Cohaus. Meteorological data collected includes:\n\nTdry Dry-bulb Temperature (\\({circ}\\)C)\nAT Ambient Temperature (\\({circ}\\)C)\nRH Relative Humidity (%)\nSpd Wind Speed (m/s)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Processing</span>"
    ]
  },
  {
    "objectID": "data.html#seasonal-dummy-variables",
    "href": "data.html#seasonal-dummy-variables",
    "title": "2  Data Processing",
    "section": "2.4 Seasonal Dummy Variables",
    "text": "2.4 Seasonal Dummy Variables\nI have created a number of boolean dummy variables to capture seasonal effects. These include:\n\nYear, Quarter, Month, Day, Week and Hour\nsunUp determines whether observation time is between sunrise and sunset on that day. Times have been pulled from the suncalc package\nworkDay is TRUE if the observation is on a workday (Mon-Fri) and not an Auckland public holiday. Auckland public holidays are sourced from the NZ government’s MBIE website.\n\n$$ Summary Statistics\n\nsummary(cohaus_wide)\n\n      dttm                          APT               CS        \n Min.   :2022-01-01 00:00:00   Min.   : 1.437   Min.   :0.1720  \n 1st Qu.:2022-07-17 11:45:00   1st Qu.: 2.986   1st Qu.:0.2830  \n Median :2023-01-30 23:30:00   Median : 4.520   Median :0.4136  \n Mean   :2023-01-30 23:30:00   Mean   : 5.174   Mean   :0.5655  \n 3rd Qu.:2023-08-16 11:15:00   3rd Qu.: 6.335   3rd Qu.:0.7350  \n Max.   :2024-02-29 23:00:00   Max.   :26.585   Max.   :3.8398  \n                                                                \n       EV                HP                Gen               Use        \n Min.   :0.01799   Min.   : 0.06398   Min.   : 0.0000   Min.   : 1.944  \n 1st Qu.:0.04297   1st Qu.: 0.11800   1st Qu.: 0.0000   1st Qu.: 4.512  \n Median :0.04398   Median : 0.12000   Median : 0.1826   Median : 6.853  \n Mean   :0.50713   Mean   : 1.43564   Mean   : 5.7007   Mean   : 7.683  \n 3rd Qu.:0.13081   3rd Qu.: 1.32455   3rd Qu.: 9.6382   3rd Qu.:10.063  \n Max.   :7.26914   Max.   :10.95870   Max.   :44.8546   Max.   :32.294  \n                                                                        \n     NetUse            Import           Export         sunUp       workDay     \n Min.   :-34.646   Min.   : 0.000   Min.   : 0.000   FALSE:9289   FALSE: 6120  \n 1st Qu.: -1.727   1st Qu.: 0.000   1st Qu.: 0.000   TRUE :9671   TRUE :12840  \n Median :  3.542   Median : 3.542   Median : 0.000                             \n Mean   :  1.982   Mean   : 4.807   Mean   : 2.825                             \n 3rd Qu.:  7.530   3rd Qu.: 7.530   3rd Qu.: 1.727                             \n Max.   : 32.294   Max.   :32.294   Max.   :34.646                             \n                                                                               \n      Tdry             RH             Spd               AT         \n Min.   : 2.00   Min.   : 30.0   Min.   : 0.000   Min.   :-0.2804  \n 1st Qu.:13.60   1st Qu.: 69.0   1st Qu.: 1.400   1st Qu.:11.7558  \n Median :16.70   Median : 82.0   Median : 2.800   Median :15.4950  \n Mean   :16.56   Mean   : 79.9   Mean   : 2.948   Mean   :15.5094  \n 3rd Qu.:19.90   3rd Qu.: 93.0   3rd Qu.: 4.200   3rd Qu.:19.5512  \n Max.   :29.20   Max.   :100.0   Max.   :11.400   Max.   :29.4112  \n                                                                   \n      hour            week           month      quarter       year     \n H00    :  790   W01    :  504   M01    :2232   Q01:5760   Y2022:8760  \n H01    :  790   W02    :  504   M02    :2040   Q02:4368   Y2023:8760  \n H02    :  790   W03    :  504   M03    :1488   Q03:4416   Y2024:1440  \n H03    :  790   W04    :  504   M05    :1488   Q04:4416               \n H04    :  790   W05    :  504   M07    :1488                          \n H05    :  790   W06    :  504   M08    :1488                          \n (Other):14220   (Other):15936   (Other):8736                          \n block    \n OP:6320  \n P1:2370  \n P2:2370  \n S1:5530  \n S2:2370  \n          \n          \n\nyrsum &lt;- cohaus_wide |&gt;\n  group_by(year(dttm)) |&gt; \n  summarise(Use = sum(Use),\n            Gen = -sum(Gen),\n            NetUse = sum(NetUse),\n            Import = sum(Import),\n            Export = -sum(Export)\n            ) |&gt; \n  rename(Yr = `year(dttm)`) |&gt;\n  mutate(NetImport = Import + Export) |&gt; \n  mutate(across(where(is.numeric) & !Yr, ~ round(., digits = 1))) |&gt; \n  print()\n\n# A tibble: 3 × 7\n     Yr    Use     Gen NetUse Import  Export NetImport\n  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1  2022 67027  -49749. 17278. 42824. -25546.    17278.\n2  2023 70023. -46400. 23623. 44407  -20784.    23623.\n3  2024  8613. -11935. -3323.  3904.  -7226.    -3323.\n\nqtrsum &lt;- cohaus_wide |&gt;\n  filter(dttm &gt;= dmy(01012022),\n         dttm &lt; dmy(01012023)) |&gt; \n  group_by(yearquarter(dttm)) |&gt; \n  summarise(Use = sum(Use),\n            Gen = -sum(Gen),\n            NetUse = sum(NetUse),\n            Import = sum(Import),\n            Export = -sum(Export)\n            ) |&gt; \n  rename(YrQtr = `yearquarter(dttm)`) |&gt;\n  mutate(NetImport = Import + Export) |&gt; \n  mutate(across(where(is.numeric) & !YrQtr, ~ round(., digits = 1))) |&gt; \n  print()\n\n# A tibble: 4 × 7\n    YrQtr    Use     Gen NetUse Import  Export NetImport\n    &lt;qtr&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1 2022 Q1 13280. -17209. -3929.  6709. -10637.    -3929.\n2 2022 Q2 17386.  -8186.  9200. 12346.  -3147.     9200.\n3 2022 Q3 21030.  -8584. 12446  15375.  -2929.    12446 \n4 2022 Q4 15331. -15770   -439.  8394.  -8833.     -439.\n\nmthsum &lt;- cohaus_wide |&gt;\n  filter(dttm &gt;= dmy(01042022),\n         dttm &lt; dmy(01072022)) |&gt; \n  group_by(yearmonth(dttm)) |&gt; \n  summarise(Use = sum(Use),\n            Gen = -sum(Gen),\n            NetUse = sum(NetUse),\n            Import = sum(Import),\n            Export = -sum(Export)\n            ) |&gt; \n  rename(YrMth = `yearmonth(dttm)`) |&gt;\n  mutate(NetImport = Import + Export) |&gt; \n  mutate(across(where(is.numeric) & !YrMth, ~ round(., digits = 1))) |&gt; \n  print()\n\n# A tibble: 3 × 7\n     YrMth   Use    Gen NetUse Import Export NetImport\n     &lt;mth&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1 2022 Apr 4958. -3588.  1370.  3084. -1714.     1370.\n2 2022 May 5712. -2670.  3042.  4072. -1030.     3042.\n3 2022 Jun 6716. -1928   4788.  5191.  -403.     4788.\n\ndaysum &lt;- cohaus_wide |&gt;\n  filter(dttm &gt;= dmy(01042022),\n         dttm &lt; dmy(01052022)) |&gt; \n  group_by(day(dttm)) |&gt; \n  summarise(Use = sum(Use),\n            Gen = -sum(Gen),\n            NetUse = sum(NetUse),\n            Import = sum(Import),\n            Export = -sum(Export)\n            ) |&gt; \n  rename(Day = `day(dttm)`) |&gt;\n  mutate(NetImport = Import + Export) |&gt; \n  mutate(across(where(is.numeric) & !Day, ~ round(., digits = 1))) |&gt; \n  print()\n\n# A tibble: 30 × 7\n     Day   Use    Gen NetUse Import Export NetImport\n   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1     1  164. -137.    26.5   91    -64.5      26.5\n 2     2  157. -136.    20.9   89.7  -68.7      20.9\n 3     3  174. -154     20.1  107.   -86.7      20.1\n 4     4  159. -108.    51.2  110.   -58.3      51.2\n 5     5  156.  -64.2   92.2  109.   -17        92.2\n 6     6  172. -124.    47.9  102.   -54.6      47.9\n 7     7  180. -168.    12.6  106.   -93.3      12.6\n 8     8  152. -116     36.5   92.3  -55.8      36.5\n 9     9  164. -174.    -9.5   89.8  -99.3      -9.5\n10    10  165. -168.    -3.5   93.2  -96.6      -3.5\n# ℹ 20 more rows\n\nhoursum &lt;- cohaus_wide |&gt;\n  filter(dttm &gt;= dmy(01042022),\n         dttm &lt; dmy(02042022)) |&gt; \n  group_by(hour(dttm)) |&gt; \n  summarise(Use = sum(Use),\n            Gen = -sum(Gen),\n            NetUse = sum(NetUse),\n            Import = sum(Import),\n            Export = -sum(Export)\n            ) |&gt; \n  rename(Hour = `hour(dttm)`) |&gt;\n  mutate(NetImport = Import + Export) |&gt; \n  mutate(across(where(is.numeric) & !Hour, ~ round(., digits = 1))) |&gt; \n  print()\n\n# A tibble: 24 × 7\n    Hour   Use   Gen NetUse Import Export NetImport\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1     0   5.1   0      5.1    5.1      0       5.1\n 2     1   4.3   0      4.3    4.3      0       4.3\n 3     2   4.2   0      4.2    4.2      0       4.2\n 4     3   4.1   0      4.1    4.1      0       4.1\n 5     4  11.2   0     11.2   11.2      0      11.2\n 6     5   3     0      3      3        0       3  \n 7     6   3.1   0      3.1    3.1      0       3.1\n 8     7   5.2  -0.1    5.1    5.1      0       5.1\n 9     8   7.6  -2.1    5.5    5.5      0       5.5\n10     9   8.7  -6.1    2.6    2.6      0       2.6\n# ℹ 14 more rows",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Processing</span>"
    ]
  },
  {
    "objectID": "exploration.html",
    "href": "exploration.html",
    "title": "3  Exploration",
    "section": "",
    "text": "3.1 Time Series Display\nHaving cleaned the data, I have coerced to be a tsibble object and am using Hydman’s fpp3package to explore the data.\nThe first three tome series displays are for the month of January 2022 and look at the hourly Usage, Generation and Net Usage for Cohaus. The ACF plots for all three show a strong diurnal autocorrelation pattern.\nlibrary(tidyverse)\n\ncohaus_ts |&gt; \n  filter(dttm &lt; dmy(\"01022022\")) |&gt;\n  gg_tsdisplay(Use, plot_type = \"histogram\") + \n  labs(title = \"Cohaus Electricity Usage for Jan 2022 (kWh)\")\n\n\n\n\n\n\n\ncohaus_ts |&gt; \n  filter(dttm &lt; dmy(\"01022022\")) |&gt;\n  gg_tsdisplay(Gen, plot_type = \"histogram\") + \n  labs(title = \"Cohaus Electricity Generation for Jan 2022 (kWh)\")\n\n\n\n\n\n\n\ncohaus_ts |&gt; \n  filter(dttm &lt; dmy(\"01022022\")) |&gt;\n  gg_tsdisplay(NetUse, plot_type = \"histogram\") + \n  labs(title = \"Cohaus Electricity Net Usage\",\n        subtitle = \"January 2022\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploration</span>"
    ]
  },
  {
    "objectID": "exploration.html#pairs-plot",
    "href": "exploration.html#pairs-plot",
    "title": "3  Exploration",
    "section": "3.2 Pairs Plot",
    "text": "3.2 Pairs Plot\nAgain for the month of January 2024, I have created a pairs plot of Usage, Generation and Temperature (dry bulb). There appears to be significant correlation between all three variables - which makes sense - we associate warm temperatures with sunshine (and hence generation) and residential energy consumption tends to happen during the day (in fact it peaks in the morning and evenings).\n\ncohaus_wide |&gt; \n  filter(month == \"M01\",\n         year == \"Y2024\") |&gt; \n  select(Use,Gen,AT) |&gt; \n  ggpairs(progress = FALSE) +\n  labs(title = \"Cohaus Pairs Plot for Use, Gen and Temp\",\n       subtitle = \"January 2024\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploration</span>"
    ]
  },
  {
    "objectID": "exploration.html#electricity-generation",
    "href": "exploration.html#electricity-generation",
    "title": "3  Exploration",
    "section": "3.3 Electricity Generation",
    "text": "3.3 Electricity Generation\n\ncohaus_ts |&gt;\n  ggplot(aes(x = hour(dttm), y = Gen)) +\n  geom_jitter(alpha = 0.1, col = \"orange\") +\n  stat_summary(fun = mean, geom = \"line\", col = \"red\") +\n  facet_wrap(~quarter, ncol = 2) +  \n  labs(title = \"Cohaus Electricity Generation (kWh)\")\n\n\n\n\n\n\n\n\n\ncohaus_ts |&gt;\n  ggplot(aes(x = Use, color = quarter)) +\n  geom_density(alpha = 0.1) +\n  facet_wrap(~workDay, ncol = 4, labeller = \"label_both\") +\n  labs(title = \"Cohaus Electricity Usage Density (kWh)\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploration</span>"
    ]
  },
  {
    "objectID": "exploration.html#electricity-usage",
    "href": "exploration.html#electricity-usage",
    "title": "3  Exploration",
    "section": "3.4 Electricity Usage",
    "text": "3.4 Electricity Usage\nHere we look at the components of Cohaus usage. The common services (CS) are relatively constant throughout the day, whereas the apartment (APT) usage peaks in the morning and evening. The EV charging (EV) peaks in the evening. The heat pump is used to centrally heat water that is utilised by each of the apartments at Cohaus. Note that heat pump usage peaks in the morning and late afternoon as the centralised hot water is scheduled to come on each day and is well insulated between these times.\n\ncohaus_wide |&gt;\n  pivot_longer(cols = c(APT,CS,EV,HP), \n               names_to = \"Measure\",\n               values_to = \"kWh\") |&gt;\n  ggplot(aes(x = yearmonth(dttm), y = kWh, fill = Measure)) +\n  geom_col()\n\n\n\n\n\n\n\ncohaus_ts |&gt;\n  ggplot(aes(x = hour(dttm), y = APT)) +\n  geom_jitter(alpha = 0.1, col = \"orange\") +\n  facet_grid(quarter~workDay, labeller = \"label_both\") +\n  labs(title = \"Cohaus APT Electricity Use (kWh)\")\n\n\n\n\n\n\n\ncohaus_ts |&gt; \n  ggplot(aes(x = hour(dttm), y = EV)) +\n  geom_jitter(alpha = 0.1, col = \"green\") +\n  labs(title = \"Cohaus EV Electricity Use (kWh)\")\n\n\n\n\n\n\n\ncohaus_ts |&gt;\n  ggplot(aes(x = hour(dttm), y = HP)) +\n  geom_jitter(alpha = 0.05, col = \"red\") +\n  facet_wrap(~quarter, ncol = 2) +\n  labs(title = \"Cohaus Heat Pump Electricity Use (kWh)\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploration</span>"
    ]
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "4  Models",
    "section": "",
    "text": "4.1 Seasonal Average\nProbably the simplest (and most intuitive) model to implement is a seasonal average model. This model calculates the arithmetic mean of NetUse by quarter and hour and applies this as our prediction of net energy consumption for our test data set. It is an ANOVA of sorts, but with a time series twist, and would be a logical place to start when considering the impact of time on energy consumption and generation.\navg &lt;- train |&gt; \n  group_by(quarter,hour) |&gt; \n  summarise(avg = mean(NetUse), .groups = \"rowwise\")\n\navg |&gt; mutate(avg = round(avg,2)) |&gt; \n  pivot_wider(names_from = quarter, values_from = avg) |&gt; \n  print()\n\n# A tibble: 24 × 5\n   hour    Q01   Q02   Q03   Q04\n   &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 H00    4.91  4.85  5.62  5.4 \n 2 H01    4.15  4.15  4.86  4.16\n 3 H02    3.44  4.09  4.9   3.52\n 4 H03    3.26  4.07  4.78  3.28\n 5 H04    8.01 11.5  12.6  11.2 \n 6 H05    3.04  3.61  4.2   3.2 \n 7 H06    3.09  4.12  4.85  2.87\n 8 H07    3.76  5.59  6.32  2.01\n 9 H08    0.9   3.39  3.85 -1.69\n10 H09   -4.28 -0.69 -0.13 -6.55\n# ℹ 14 more rows\n\nyhat_avg &lt;- test |&gt; \n  left_join(avg, by = c(\"hour\", \"quarter\")) |&gt; pull(avg)\n  \ntest |&gt; \n  ggplot(aes(y = NetUse, x = yhat_avg)) +\n  geom_point(alpha = 0.1, color = \"blue\") +\n  geom_abline(color = \"black\") +\n  theme_bw() +\n  labs(title = \"Seasonal Average Model\",\n       x = \"Predicted\",\n       y = \"Actual\")\n\n\n\n\n\n\n\n# mse_pct(test$NetUse, yhat_avg)\nWhile simple, the Seasonal Average Model has a mean squared error of 29.7% of the variance of the actual values in the test dataset. This is a good starting point for our analysis, and we will now consider more complex models that may provide better predictions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "models.html#ols-linear-regression",
    "href": "models.html#ols-linear-regression",
    "title": "4  Models",
    "section": "4.2 OLS Linear Regression",
    "text": "4.2 OLS Linear Regression\nThe next model we will consider is a simple linear regression model. This model will use all of the variables in our data set to predict NetUse. We will use the lm() function in R to fit the model and then use the predict() function to generate predictions for our test data set. We will then calculate the mean squared error as a percentage of the variance of the actual values in the test dataset. Note that the climatological factors such as dry temperature (Tdry), and relative humidity (RH) are included in the model, and we assume that they are known. In reality, these would be forecasted values, and we would need to consider the uncertainty in these forecasts when making predictions. Other variates such as sunUp and workDay are binary variables that are known in advance.\n\nols &lt;- lm(NetUse ~ ., data = train)\n\nsummary(ols)\n\n\nCall:\nlm(formula = NetUse ~ ., data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-23.6291  -2.9336   0.0607   3.0091  22.5722 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -9.039528   0.480938 -18.796  &lt; 2e-16 ***\nsunUpTRUE   -0.941970   0.204322  -4.610 4.06e-06 ***\nworkDayTRUE -0.189508   0.084461  -2.244 0.024864 *  \nTdry        -0.295028   0.013523 -21.817  &lt; 2e-16 ***\nRH           0.192906   0.003655  52.775  &lt; 2e-16 ***\nhourH01     -1.010703   0.273283  -3.698 0.000218 ***\nhourH02     -1.504223   0.273350  -5.503 3.80e-08 ***\nhourH03     -1.757583   0.273433  -6.428 1.33e-10 ***\nhourH04      4.991341   0.273504  18.250  &lt; 2e-16 ***\nhourH05     -2.283014   0.273567  -8.345  &lt; 2e-16 ***\nhourH06     -1.978511   0.273782  -7.227 5.19e-13 ***\nhourH07     -0.270190   0.292706  -0.923 0.355981    \nhourH08     -1.508674   0.340744  -4.428 9.60e-06 ***\nhourH09     -4.543530   0.341089 -13.321  &lt; 2e-16 ***\nhourH10     -5.920559   0.343142 -17.254  &lt; 2e-16 ***\nhourH11     -6.768134   0.345728 -19.576  &lt; 2e-16 ***\nhourH12     -7.621006   0.347355 -21.940  &lt; 2e-16 ***\nhourH13     -8.361509   0.348598 -23.986  &lt; 2e-16 ***\nhourH14     -7.713602   0.348927 -22.107  &lt; 2e-16 ***\nhourH15     -4.422326   0.348345 -12.695  &lt; 2e-16 ***\nhourH16      3.525592   0.346995  10.160  &lt; 2e-16 ***\nhourH17      6.845233   0.344840  19.850  &lt; 2e-16 ***\nhourH18      9.737877   0.299412  32.523  &lt; 2e-16 ***\nhourH19      7.867882   0.289387  27.188  &lt; 2e-16 ***\nhourH20      7.369737   0.279925  26.328  &lt; 2e-16 ***\nhourH21      6.492781   0.273716  23.721  &lt; 2e-16 ***\nhourH22      4.852809   0.273420  17.749  &lt; 2e-16 ***\nhourH23      2.148023   0.273287   7.860 4.10e-15 ***\nquarterQ02   2.702112   0.124164  21.762  &lt; 2e-16 ***\nquarterQ03   2.939880   0.146201  20.109  &lt; 2e-16 ***\nquarterQ04  -0.429109   0.132951  -3.228 0.001251 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.858 on 15137 degrees of freedom\nMultiple R-squared:  0.7204,    Adjusted R-squared:  0.7199 \nF-statistic:  1300 on 30 and 15137 DF,  p-value: &lt; 2.2e-16\n\nyhat_ols &lt;- predict(ols, newdata = test)\n\ntest |&gt; \n  ggplot(aes(y = NetUse, x = yhat_ols)) +\n  geom_point(alpha = 0.1, color = \"blue\") +\n  geom_abline(color = \"black\") +\n  theme_bw() +\n  labs(title = \"OLS Regression Model\",\n       x = \"Predicted\",\n       y = \"Actual\")\n\n\n\n\n\n\n\n# mse_pct(test$NetUse, yhat_ols)\n\nIn this case, the OLS Regression Model has a mean squared error of 29.6% of the variance of the actual values in the test dataset, a slight improvement over the Seasonal Average Model.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "models.html#lasso-regression",
    "href": "models.html#lasso-regression",
    "title": "4  Models",
    "section": "4.3 Lasso Regression",
    "text": "4.3 Lasso Regression\n\ntrain_modelframe &lt;- model.frame(NetUse ~ ., data = train)\ntrain_X &lt;- model.matrix(NetUse ~ ., data = train_modelframe)\ndim(train_X)\n\n[1] 15168    31\n\ntest_modelframe &lt;- model.frame(NetUse ~ ., data = test)\ntest_X &lt;- model.matrix(NetUse ~ ., data = test_modelframe)\ndim(test_X)\n\n[1] 3792   31\n\nsuppressMessages(library(glmnet))\n\nlasso_cv &lt;- cv.glmnet(train_X, train$NetUse)\nplot(lasso_cv)\n\n\n\n\n\n\n\nlasso &lt;- glmnet(train_X, train$NetUse, lambda = lasso_cv$lambda.1se)\n\npredict(lasso, type = \"coef\")\n\n32 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s0\n(Intercept) -9.29638731\n(Intercept)  .         \nsunUpTRUE   -1.58412934\nworkDayTRUE -0.01171215\nTdry        -0.31078557\nRH           0.19168061\nhourH01      .         \nhourH02     -0.38190104\nhourH03     -0.63762778\nhourH04      5.30016906\nhourH05     -1.16542948\nhourH06     -0.83065268\nhourH07      0.37166076\nhourH08      .         \nhourH09     -2.74628902\nhourH10     -4.11215284\nhourH11     -4.95153824\nhourH12     -5.79951897\nhourH13     -6.53764733\nhourH14     -5.88912654\nhourH15     -2.59906951\nhourH16      4.53504514\nhourH17      7.84797530\nhourH18     10.45001021\nhourH19      8.49888142\nhourH20      7.89043227\nhourH21      6.82793601\nhourH22      5.18485606\nhourH23      2.47709036\nquarterQ02   2.26582819\nquarterQ03   2.45885640\nquarterQ04  -0.41333328\n\nyhat_lasso &lt;- predict(lasso, newx = test_X)\n\ntest |&gt; \n  ggplot(aes(y = NetUse, x = yhat_lasso)) +\n  geom_point(alpha = 0.1, color = \"blue\") +\n  geom_abline(color = \"black\") +\n  theme_bw() +\n  labs(title = \"Lasso Regression Model\",\n       x = \"Predicted\",\n       y = \"Actual\")\n\n\n\n\n\n\n\n# mse_pct(test$NetUse, yhat_lasso)\n\nThis model has a mean squared error of 30% of the variance of the actual values in the test dataset, a slight improvement over the OLS Regression Model. The Lasso Regression Model has the advantage of performing variable selection, which can be useful when there are many variables in the model. However, in this case, the Lasso Regression Model has selected nearly all of the variables hour, quarter, sunUp, workDay, Tdry, and RH as important predictors of NetUse.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "models.html#lasso-regression-with-interactions",
    "href": "models.html#lasso-regression-with-interactions",
    "title": "4  Models",
    "section": "4.4 Lasso Regression with Interactions",
    "text": "4.4 Lasso Regression with Interactions\n\nintformula &lt;- NetUse ~ (hour + quarter) * \n                        (sunUp + workDay + Tdry + RH)\n\ntrain_modelframe &lt;- model.frame(intformula, data = train)\ntrain_X_int &lt;- model.matrix(intformula, train_modelframe)\ndim(train_X_int)\n\n[1] 15168   135\n\ntest_modelframe &lt;- model.frame(intformula, data = test)\ntest_X_int &lt;- model.matrix(intformula, test_modelframe)\ndim(test_X_int)\n\n[1] 3792  135\n\ncv_lasso_int &lt;- cv.glmnet(train_X_int, train$NetUse)\nplot(cv_lasso_int)\n\n\n\n\n\n\n\nlasso_int &lt;- glmnet(train_X_int, train$NetUse, lambda = cv_lasso_int$lambda.1se)\n\nyhat_lasso_int &lt;- predict(lasso_int, newx = test_X_int)\n\ntest |&gt; \n  ggplot(aes(y = NetUse, x = yhat_lasso_int)) +\n  geom_jitter(alpha = 0.1, color = \"blue\") +\n  geom_abline(color = \"black\") +\n  theme_bw() +\n  labs(title = \"Lasso with Interactions Model\",\n       x = \"Predicted\",\n       y = \"Actual\")\n\n\n\n\n\n\n\n# mse_pct(test$NetUse, yhat_lasso_int)\n\nThis model has a mean squared error of 20% of the variance of the actual values in the test dataset, a significant improvement over the Lasso Regression Model, and our best predictive model so far.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "models.html#ridge-regression",
    "href": "models.html#ridge-regression",
    "title": "4  Models",
    "section": "4.5 Ridge Regression",
    "text": "4.5 Ridge Regression\n\nfit &lt;- glmnet(train_X, train$NetUse, alpha = 0) # alpha = 0 for ridge\nplot(fit)\n\n\n\n\n\n\n\nxval &lt;- cv.glmnet(train_X, train$NetUse, alpha = 0)\nplot(xval)\n\n\n\n\n\n\n\ncoef(fit, s = xval$lambda.1se)\n\n32 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s1\n(Intercept) -6.44565528\n(Intercept)  .         \nsunUpTRUE   -1.93742490\nworkDayTRUE -0.17231436\nTdry        -0.31632086\nRH           0.17351202\nhourH01     -1.10712330\nhourH02     -1.54743767\nhourH03     -1.77510537\nhourH04      4.35438917\nhourH05     -2.24861508\nhourH06     -1.92654481\nhourH07      0.03061436\nhourH08     -0.68790006\nhourH09     -3.50782902\nhourH10     -4.83844681\nhourH11     -5.66901896\nhourH12     -6.47234462\nhourH13     -7.16806815\nhourH14     -6.58672682\nhourH15     -3.59045816\nhourH16      3.64450843\nhourH17      6.69678840\nhourH18      8.97743167\nhourH19      7.23555653\nhourH20      6.67747658\nhourH21      5.65350955\nhourH22      4.18494160\nhourH23      1.74485795\nquarterQ02   2.14669399\nquarterQ03   2.32673827\nquarterQ04  -0.67483024\n\nyhat_ridge &lt;- predict(fit, newx = test_X, s = xval$lambda.1se)\n\ntest |&gt; \n  ggplot(aes(y = NetUse, x = yhat_lasso)) +\n  geom_point(alpha = 0.1, color = \"blue\") +\n  geom_abline(color = \"black\") +\n  theme_bw() +\n  labs(title = \"Ridge Regression Model\",\n       x = \"Predicted\",\n       y = \"Actual\")\n\n\n\n\n\n\n\n# mse_pct(test$NetUse, yhat_ridge)\n\nThis model has a mean squared error of 30.4% of the variance of the actual values in the test dataset, a slight improvement over the Lasso Regression Model. The Ridge Regression Model has the advantage of being less sensitive to multicollinearity than the Lasso Regression Model.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "models.html#regression-tree",
    "href": "models.html#regression-tree",
    "title": "4  Models",
    "section": "4.6 Regression Tree",
    "text": "4.6 Regression Tree\n\ntree &lt;- rpart(NetUse ~ ., \n              data = train,\n              control = rpart.control(cp = 0.0001))\n\ncptable &lt;- as_tibble(tree$cptable)\n\nx1se &lt;- cptable |&gt; \n  mutate(x1se = xerror + xstd) |&gt;\n  filter(xerror == min(xerror)) |&gt;\n  pull(x1se)\n\ncp_1se &lt;- cptable |&gt;\n  filter(xerror &lt; x1se) |&gt;\n  filter(xerror == max(xerror))\n\ncp_1se\n\n# A tibble: 1 × 5\n        CP nsplit `rel error` xerror    xstd\n     &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 0.000276     93       0.168  0.188 0.00306\n\ncptable |&gt; \n  ggplot(aes(x = nsplit, y = xerror)) +\n  geom_line() +\n  geom_point() +\n  geom_vline(xintercept = cp_1se$nsplit, color = \"red\") +\n  labs(title = \"Cost Complexity Pruning\",\n       x = \"Number of Splits\",\n       y = \"Cross-Validation Error\") +\n  theme_bw()\n\n\n\n\n\n\n\nprune_tree &lt;- prune(tree, cp = cp_1se$CP)\n\nhead(prune_tree$cptable)\n\n          CP nsplit rel error    xerror        xstd\n1 0.44019051      0 1.0000000 1.0001951 0.014488806\n2 0.09184008      1 0.5598095 0.5600058 0.008079358\n3 0.07250195      2 0.4679694 0.4696495 0.007022138\n4 0.03266437      3 0.3954675 0.3971612 0.006400727\n5 0.02604336      4 0.3628031 0.3634131 0.006056158\n6 0.02301931      5 0.3367597 0.3398637 0.005235032\n\nyhat_pruned &lt;- predict(prune_tree, test, type = \"vector\")\n\ntest |&gt; \n  ggplot(aes(y = NetUse, x = yhat_pruned)) +\n  geom_jitter(alpha = 0.1, color = \"blue\") +\n  geom_abline(color = \"black\") +\n  theme_bw() +\n  labs(title = \"Pruned Regression Tree Model\",\n       x = \"Predicted\",\n       y = \"Actual\")\n\n\n\n\n\n\n\nmse_pct(test$NetUse, yhat_pruned)\n\n[1] 22.92341",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "models.html#random-forest",
    "href": "models.html#random-forest",
    "title": "4  Models",
    "section": "4.7 Random Forest",
    "text": "4.7 Random Forest\n\nsuppressMessages(library(ranger))\ntry(forest &lt;- ranger(NetUse ~ ., \n                     data = train,\n                     num.trees = 1000,\n                     mtry = 2,\n                     importance = \"impurity\")\n    )\n\nforest\n\nRanger result\n\nCall:\n ranger(NetUse ~ ., data = train, num.trees = 1000, mtry = 2,      importance = \"impurity\") \n\nType:                             Regression \nNumber of trees:                  1000 \nSample size:                      15168 \nNumber of independent variables:  6 \nMtry:                             2 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       13.94609 \nR squared (OOB):                  0.834429 \n\nsort(forest$variable.importance, decreasing = TRUE)/sum(forest$variable.importance)\n\n       hour          RH        Tdry       sunUp     quarter     workDay \n0.332078412 0.273992209 0.195335024 0.137876770 0.053925341 0.006792244 \n\nyhat_forest &lt;- predict(forest, test, type = \"response\")$predictions\n\ntest |&gt; \n  ggplot(aes(y = NetUse, x = yhat_forest)) +\n  geom_jitter(alpha = 0.1, color = \"blue\") +\n  geom_abline(color = \"black\") +\n  theme_bw() +\n  labs(title = \"Random Forest Model\",\n       x = \"Predicted\",\n       y = \"Actual\")\n\n\n\n\n\n\n\n# mse_pct(test$NetUse, yhat_forest)\n\nThis model has a mean squared error of 21.5% of the variance of the actual values in the test dataset, a slight improvement over the Pruned Regression Tree Model. The Random Forest Model has the advantage of being less sensitive to overfitting than the Pruned Regression Tree Model.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "models.html#xgboost",
    "href": "models.html#xgboost",
    "title": "4  Models",
    "section": "4.8 XGBoost",
    "text": "4.8 XGBoost\n\nsuppressMessages(library(xgboost))\n\nxgb_cv &lt;- xgb.cv(data = train_X, \n                 label = train$NetUse, \n                 nrounds = 500, \n                 nfold = 10,\n                 eta = 0.1,\n                 early_stopping_rounds = 10,\n                 objective = \"reg:squarederror\", \n                 print_every_n = 10)\n\n[1] train-rmse:8.775789+0.016969    test-rmse:8.784456+0.157858 \nMultiple eval metrics are present. Will use test_rmse for early stopping.\nWill train until test_rmse hasn't improved in 10 rounds.\n\n[11]    train-rmse:5.363457+0.009351    test-rmse:5.448735+0.111744 \n[21]    train-rmse:4.311584+0.014339    test-rmse:4.467683+0.092786 \n[31]    train-rmse:3.897520+0.011619    test-rmse:4.108626+0.085632 \n[41]    train-rmse:3.701552+0.009329    test-rmse:3.956388+0.083490 \n[51]    train-rmse:3.588474+0.007918    test-rmse:3.880037+0.082436 \n[61]    train-rmse:3.516325+0.009961    test-rmse:3.838205+0.084518 \n[71]    train-rmse:3.461216+0.009838    test-rmse:3.812638+0.085630 \n[81]    train-rmse:3.418219+0.009353    test-rmse:3.794910+0.085900 \n[91]    train-rmse:3.377933+0.008209    test-rmse:3.782893+0.085973 \n[101]   train-rmse:3.344584+0.010214    test-rmse:3.776263+0.085549 \n[111]   train-rmse:3.316151+0.010613    test-rmse:3.771457+0.084734 \n[121]   train-rmse:3.288372+0.011347    test-rmse:3.769233+0.084370 \n[131]   train-rmse:3.264286+0.013652    test-rmse:3.766216+0.082609 \n[141]   train-rmse:3.239969+0.013207    test-rmse:3.766986+0.082357 \nStopping. Best iteration:\n[133]   train-rmse:3.259121+0.012159    test-rmse:3.765773+0.081979\n\nxgb &lt;- xgboost(data = train_X, \n               label = train$NetUse, \n               nrounds = xgb_cv$best_iteration, \n               eta = 0.1,\n               objective = \"reg:squarederror\",\n               print_every_n = 10)\n\n[1] train-rmse:8.775400 \n[11]    train-rmse:5.372372 \n[21]    train-rmse:4.319832 \n[31]    train-rmse:3.911859 \n[41]    train-rmse:3.719267 \n[51]    train-rmse:3.609865 \n[61]    train-rmse:3.535674 \n[71]    train-rmse:3.491409 \n[81]    train-rmse:3.442929 \n[91]    train-rmse:3.415678 \n[101]   train-rmse:3.372026 \n[111]   train-rmse:3.341263 \n[121]   train-rmse:3.317808 \n[131]   train-rmse:3.291732 \n[133]   train-rmse:3.287047 \n\nyhat_xgb &lt;- predict(xgb, newdata = test_X)\n\ntest |&gt; \n  ggplot(aes(y = NetUse, x = yhat_pruned)) +\n  geom_jitter(alpha = 0.1, color = \"blue\") +\n  geom_abline(color = \"black\") +\n  theme_bw() +\n  labs(title = \"XGBoost Model\",\n       x = \"Predicted\",\n       y = \"Actual\")\n\n\n\n\n\n\n\n# mse_pct(test$NetUse, yhat_xgb)\n\nThis model has a mean squared error of 20.9% of the variance of the actual values in the test dataset, a slight improvement over the Random Forest Model. The XGBoost Model has the advantage of being less sensitive to overfitting than the Random Forest Model.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "models.html#seasonal-ets",
    "href": "models.html#seasonal-ets",
    "title": "4  Models",
    "section": "4.9 Seasonal ETS",
    "text": "4.9 Seasonal ETS\n\nfit_ets &lt;- train_ts |&gt; \n  model(ETS(NetUse ~ error(\"A\") + trend(\"N\") + season(\"A\")))\n\nfit_ets |&gt; report()\n\nSeries: NetUse \nModel: ETS(A,N,A) \n  Smoothing parameters:\n    alpha = 0.586248 \n    gamma = 0.1142449 \n\n  Initial states:\n      l[0]     s[0]    s[-1]   s[-2]    s[-3]    s[-4]   s[-5]    s[-6]\n -7.782323 5.198271 6.031678 5.92693 9.079136 6.682808 7.24666 1.695157\n      s[-7]     s[-8]     s[-9]    s[-10]    s[-11]    s[-12]    s[-13]\n -0.7263089 -7.259734 -10.02647 -10.67955 -11.26348 -9.826209 -7.847681\n    s[-14]     s[-15]   s[-16]   s[-17]   s[-18]   s[-19]    s[-20]   s[-21]\n -3.811655 -0.4996266 1.516251 2.047417 1.334611 9.058746 0.9822853 2.134487\n   s[-22]    s[-23]\n 2.591731 0.4145526\n\n  sigma^2:  13.2084\n\n     AIC     AICc      BIC \n185195.8 185195.9 185401.8 \n\nyhat_ets &lt;- fit_ets |&gt; forecast(h = nrow(test_ts)) |&gt; as_tibble() |&gt; pull(.mean)\n\ntest_ts |&gt; \n  ggplot(aes(y = NetUse, x = yhat_ets)) +\n  geom_point(alpha = 0.1, color = \"blue\") +\n  geom_abline(color = \"black\") +\n  theme_bw() +\n  labs(title = \"Seasonal ETS Model\",\n       x = \"Predicted\",\n       y = \"Actual\")\n\n\n\n\n\n\n\n# mse_pct(test_ts$NetUse, yhat_ets)\n\nThis ETS Model has a mean squared error of 49.9% of the variance of the actual values in the test dataset, permornimg much worse that all of the models presented so far.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "models.html#arimax",
    "href": "models.html#arimax",
    "title": "4  Models",
    "section": "4.10 ARIMAx",
    "text": "4.10 ARIMAx\n\n# fit arimax model with exogenous variables, let fabletools find best pdqs\n# fit_arimax &lt;- train_ts |&gt; \n#   model(ARIMA(NetUse ~ xreg(sunUp + workDay + Tdry + RH)))\n# \n# fit_arimax |&gt; report()\n# \n\nfit_arimax &lt;- train_ts |&gt; \n  model(ARIMA(NetUse ~ pdq(1,0,3) + PDQ(2,1,0) + xreg(sunUp + workDay + Tdry + RH)))\n\nfit_arimax |&gt; report()\n\nSeries: NetUse \nModel: LM w/ ARIMA(1,0,3)(2,1,0)[24] errors \n\nCoefficients:\n         ar1      ma1      ma2    ma3     sar1     sar2  sunUpTRUE  workDayTRUE\n      0.7441  -0.2083  -0.0446  0.036  -0.6053  -0.2948     1.1186      -0.2260\ns.e.  0.0144   0.0163   0.0111  0.010   0.0078   0.0078     0.5448       0.1302\n         Tdry      RH\n      -0.2034  0.1144\ns.e.   0.0302  0.0059\n\nsigma^2 estimated as 13.08:  log likelihood=-40957.97\nAIC=81937.94   AICc=81937.96   BIC=82021.82\n\nnewdata &lt;- test_ts |&gt; \n  select(dttm, hour, quarter, Tdry, RH, sunUp, workDay) |&gt; \n  as_tsibble(index = dttm)\n\nyhat_arimax &lt;- fit_arimax |&gt; forecast(newdata) |&gt; as_tibble() |&gt; pull(.mean)\n\ntest |&gt; \n  ggplot(aes(y = NetUse, x = yhat_arimax)) +\n  geom_point(alpha = 0.1, color = \"blue\") +\n  geom_abline(color = \"black\") +\n  theme_bw() +\n  labs(title = \"ARIMAx Model\",\n       x = \"Predicted\",\n       y = \"Actual\")\n\n\n\n\n\n\n\n# mse_pct(test$NetUse, yhat_arimax)\n\nThis ARIMAx Model has a mean squared error of 56% of the variance of the actual values in the test dataset, and is fighting for the wooden spoon along with the ETS Model. The ARIMAx Model has the advantage of being able to model the autocorrelation in the data, which can be useful when there are time-dependent patterns in the data (which there are). Non-stationarity is probably the reason for the poor performance of the ARIMAx Model.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "models.html#summary-of-results",
    "href": "models.html#summary-of-results",
    "title": "4  Models",
    "section": "4.11 Summary of Results",
    "text": "4.11 Summary of Results\n\nresults &lt;- tibble(Model = c(\"Seasonal Average\",\n                            \"OLS Regression\", \n                            \"Lasso Regression\", \n                            \"Pruned Regression Tree\", \n                            \"Random Forest\", \n                            \"XGBoost\", \n                            \"Lasso Regression with Interactions\", \n                            \"Ridge Regression\",\n                            \"Seasonal ETS\",\n                            \"ARIMAx\"),\n                   mse_pct = c(mse_pct(test$NetUse, yhat_avg),\n                               mse_pct(test$NetUse, yhat_ols),\n                               mse_pct(test$NetUse, yhat_lasso),\n                               mse_pct(test$NetUse, yhat_pruned),\n                               mse_pct(test$NetUse, yhat_forest),\n                               mse_pct(test$NetUse, yhat_xgb),\n                               mse_pct(test$NetUse, yhat_lasso_int),\n                               mse_pct(test$NetUse, yhat_ridge),\n                               mse_pct(test_ts$NetUse, yhat_ets),\n                               mse_pct(test$NetUse, yhat_arimax))\n                  ) |&gt; \n  mutate(mse_pct = round(mse_pct, 2)) |&gt; \n  arrange(mse_pct) |&gt; \n  print()\n\n# A tibble: 10 × 2\n   Model                              mse_pct\n   &lt;chr&gt;                                &lt;dbl&gt;\n 1 Lasso Regression with Interactions    20.0\n 2 XGBoost                               20.9\n 3 Random Forest                         21.6\n 4 Pruned Regression Tree                22.9\n 5 OLS Regression                        29.6\n 6 Seasonal Average                      29.7\n 7 Lasso Regression                      30  \n 8 Ridge Regression                      30.4\n 9 Seasonal ETS                          49.9\n10 ARIMAx                                56.0",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "5  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "models.html#pruned-regression-tree",
    "href": "models.html#pruned-regression-tree",
    "title": "4  Models",
    "section": "4.6 Pruned Regression Tree",
    "text": "4.6 Pruned Regression Tree\n\ntree &lt;- rpart(NetUse ~ ., \n              data = train,\n              control = rpart.control(cp = 0.0001))\n\ncptable &lt;- as_tibble(tree$cptable)\n\nx1se &lt;- cptable |&gt; \n  mutate(x1se = xerror + xstd) |&gt;\n  filter(xerror == min(xerror)) |&gt;\n  pull(x1se)\n\ncp_1se &lt;- cptable |&gt;\n  filter(xerror &lt; x1se) |&gt;\n  filter(xerror == max(xerror))\n\ncp_1se\n\n# A tibble: 1 × 5\n        CP nsplit `rel error` xerror    xstd\n     &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 0.000276     93       0.168  0.188 0.00306\n\ncptable |&gt; \n  ggplot(aes(x = nsplit, y = xerror)) +\n  geom_line() +\n  geom_point() +\n  geom_vline(xintercept = cp_1se$nsplit, color = \"red\") +\n  labs(title = \"Cost Complexity Pruning\",\n       x = \"Number of Splits\",\n       y = \"Cross-Validation Error\") +\n  theme_bw()\n\n\n\n\n\n\n\nprune_tree &lt;- prune(tree, cp = cp_1se$CP)\n\nhead(prune_tree$cptable)\n\n          CP nsplit rel error    xerror        xstd\n1 0.44019051      0 1.0000000 1.0001951 0.014488806\n2 0.09184008      1 0.5598095 0.5600058 0.008079358\n3 0.07250195      2 0.4679694 0.4696495 0.007022138\n4 0.03266437      3 0.3954675 0.3971612 0.006400727\n5 0.02604336      4 0.3628031 0.3634131 0.006056158\n6 0.02301931      5 0.3367597 0.3398637 0.005235032\n\nyhat_pruned &lt;- predict(prune_tree, test, type = \"vector\")\n\ntest |&gt; \n  ggplot(aes(y = NetUse, x = yhat_pruned)) +\n  geom_jitter(alpha = 0.1, color = \"blue\") +\n  geom_abline(color = \"black\") +\n  theme_bw() +\n  labs(title = \"Pruned Regression Tree Model\",\n       x = \"Predicted\",\n       y = \"Actual\")\n\n\n\n\n\n\n\n# mse_pct(test$NetUse, yhat_pruned)\n\nThis model has a mean squared error of 22.9% of the variance of the actual values in the test dataset, a slight improvement over the Lasso Regression Model. The Pruned Regression Tree Model has the advantage of being interpretable and easy to understand.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models</span>"
    ]
  }
]