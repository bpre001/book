# Models

```{r loadin, include=FALSE}

library(tidyverse)
library(readxl)
library(fpp3)
library(glmnet)
library(rpart)
library(ranger)

# read in cohaus_wide (painful wrangle is in separate wrangle.R)
# source("wrangle.R")

cohaus_wide <- read_rds("data/cohaus_wide.rds")|> 
  select(dttm, NetUse,sunUp,workDay,
         Tdry,RH,hour,quarter)

#convert to tsibble
#
#cohaus_ts <- cohaus_wide |> as_tsibble(index = dttm)

```

```{r train_test}

set.seed(123)
#test <- cohaus_wide |> sample_frac(0.2)
#train <- anti_join(cohaus_wide, test, by = "dttm")

train <- cohaus_wide |> 
  slice_head(prop = 0.8)

test <- cohaus_wide |> 
  slice_tail(n = nrow(cohaus_wide) - nrow(train))

train_ts <- train |> as_tsibble(index = dttm)
test_ts <- test |> as_tsibble(index = dttm)

train <- train |> select(-dttm)
test <- test |> select(-dttm)

dim(train); dim(test)


## Function to determine Mean Squared Error as Percentage of Variance
 
mse_pct <- function(actual, predicted) {
  100 * mean((actual - predicted)^2) / var(actual)
}

```

## Seasonal Average

Probably the simplest (and most intuitive) model to implement is a seasonal average model. This model calculates the arithmetic mean of NetUse by quarter and hour and applies this as our prediction of net energy consumption for our test data set.  It is an ANOVA of sorts, but with a time series twist, and would be a logical place to start when considering the impact of time on energy consumption and generation.

```{r avg}

avg <- train |> 
  group_by(quarter,hour) |> 
  summarise(avg = mean(NetUse), .groups = "rowwise")

avg |> mutate(avg = round(avg,2)) |> 
  pivot_wider(names_from = quarter, values_from = avg) |> 
  print()

yhat_avg <- test |> 
  left_join(avg, by = c("hour", "quarter")) |> pull(avg)
  
test |> 
  ggplot(aes(y = NetUse, x = yhat_avg)) +
  geom_point(alpha = 0.1, color = "blue") +
  geom_abline(color = "black") +
  theme_bw() +
  labs(title = "Seasonal Average Model",
       x = "Predicted",
       y = "Actual")

# mse_pct(test$NetUse, yhat_avg)

```

While simple, the Seasonal Average Model has a mean squared error of `r round(mse_pct(test$NetUse, yhat_avg),1)`% of the variance of the actual values in the test dataset.  This is a good starting point for our analysis, and we will now consider more complex models that may provide better predictions.

## OLS Linear Regression

The next model we will consider is a simple linear regression model. This model will use all of the variables in our data set to predict NetUse. We will use the lm() function in R to fit the model and then use the predict() function to generate predictions for our test data set. We will then calculate the mean squared error as a percentage of the variance of the actual values in the test dataset.  Note that the climatological factors such as dry temperature (`Tdry`), and relative humidity (`RH`) are included in the model, and we assume that they are known.  In reality, these would be forecasted values, and we would need to consider the uncertainty in these forecasts when making predictions.  Other variates such as `sunUp` and `workDay` are binary variables that are known in advance.

```{r ols}

ols <- lm(NetUse ~ ., data = train)

summary(ols)

yhat_ols <- predict(ols, newdata = test)

test |> 
  ggplot(aes(y = NetUse, x = yhat_ols)) +
  geom_point(alpha = 0.1, color = "blue") +
  geom_abline(color = "black") +
  theme_bw() +
  labs(title = "OLS Regression Model",
       x = "Predicted",
       y = "Actual")

# mse_pct(test$NetUse, yhat_ols)

```

In this case, the OLS Regression Model has a mean squared error of `r round(mse_pct(test$NetUse, yhat_ols),1)`% of the variance of the actual values in the test dataset, a slight improvement over the Seasonal Average Model.  

## Lasso Regression 

```{r lasso}

train_modelframe <- model.frame(NetUse ~ ., data = train)
train_X <- model.matrix(NetUse ~ ., data = train_modelframe)
dim(train_X)

test_modelframe <- model.frame(NetUse ~ ., data = test)
test_X <- model.matrix(NetUse ~ ., data = test_modelframe)
dim(test_X)

suppressMessages(library(glmnet))

lasso_cv <- cv.glmnet(train_X, train$NetUse)
plot(lasso_cv)

lasso <- glmnet(train_X, train$NetUse, lambda = lasso_cv$lambda.1se)

predict(lasso, type = "coef")

yhat_lasso <- predict(lasso, newx = test_X)

test |> 
  ggplot(aes(y = NetUse, x = yhat_lasso)) +
  geom_point(alpha = 0.1, color = "blue") +
  geom_abline(color = "black") +
  theme_bw() +
  labs(title = "Lasso Regression Model",
       x = "Predicted",
       y = "Actual")

# mse_pct(test$NetUse, yhat_lasso)

```

This model has a mean squared error of `r round(mse_pct(test$NetUse, yhat_lasso),1)`% of the variance of the actual values in the test dataset, a slight improvement over the OLS Regression Model.  The Lasso Regression Model has the advantage of performing variable selection, which can be useful when there are many variables in the model.  However, in this case, the Lasso Regression Model has selected nearly all of the variables `hour`, `quarter`, `sunUp`, `workDay`, `Tdry`, and `RH` as important predictors of `NetUse`.

## Lasso Regression with Interactions

```{r lasso_int}

intformula <- NetUse ~ (hour + quarter) * 
                        (sunUp + workDay + Tdry + RH)

train_modelframe <- model.frame(intformula, data = train)
train_X_int <- model.matrix(intformula, train_modelframe)
dim(train_X_int)

test_modelframe <- model.frame(intformula, data = test)
test_X_int <- model.matrix(intformula, test_modelframe)
dim(test_X_int)

cv_lasso_int <- cv.glmnet(train_X_int, train$NetUse)
plot(cv_lasso_int)

lasso_int <- glmnet(train_X_int, train$NetUse, lambda = cv_lasso_int$lambda.1se)

yhat_lasso_int <- predict(lasso_int, newx = test_X_int)

test |> 
  ggplot(aes(y = NetUse, x = yhat_lasso_int)) +
  geom_jitter(alpha = 0.1, color = "blue") +
  geom_abline(color = "black") +
  theme_bw() +
  labs(title = "Lasso with Interactions Model",
       x = "Predicted",
       y = "Actual")

# mse_pct(test$NetUse, yhat_lasso_int)

```

This model has a mean squared error of `r round(mse_pct(test$NetUse, yhat_lasso_int),1)`% of the variance of the actual values in the test dataset, a significant improvement over the Lasso Regression Model, and our best predictive model so far.

## Ridge Regression

```{r ridge}

fit <- glmnet(train_X, train$NetUse, alpha = 0) # alpha = 0 for ridge
plot(fit)

xval <- cv.glmnet(train_X, train$NetUse, alpha = 0)
plot(xval)

coef(fit, s = xval$lambda.1se)

yhat_ridge <- predict(fit, newx = test_X, s = xval$lambda.1se)

test |> 
  ggplot(aes(y = NetUse, x = yhat_lasso)) +
  geom_point(alpha = 0.1, color = "blue") +
  geom_abline(color = "black") +
  theme_bw() +
  labs(title = "Ridge Regression Model",
       x = "Predicted",
       y = "Actual")

# mse_pct(test$NetUse, yhat_ridge)

```

This model has a mean squared error of `r round(mse_pct(test$NetUse, yhat_ridge),1)`% of the variance of the actual values in the test dataset, a slight improvement over the Lasso Regression Model.  The Ridge Regression Model has the advantage of being less sensitive to multicollinearity than the Lasso Regression Model.

## Pruned Regression Tree

```{r regression_tree}

tree <- rpart(NetUse ~ ., 
              data = train,
              control = rpart.control(cp = 0.0001))

cptable <- as_tibble(tree$cptable)

x1se <- cptable |> 
  mutate(x1se = xerror + xstd) |>
  filter(xerror == min(xerror)) |>
  pull(x1se)

cp_1se <- cptable |>
  filter(xerror < x1se) |>
  filter(xerror == max(xerror))

cp_1se

cptable |> 
  ggplot(aes(x = nsplit, y = xerror)) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = cp_1se$nsplit, color = "red") +
  labs(title = "Cost Complexity Pruning",
       x = "Number of Splits",
       y = "Cross-Validation Error") +
  theme_bw()

prune_tree <- prune(tree, cp = cp_1se$CP)

head(prune_tree$cptable)

yhat_pruned <- predict(prune_tree, test, type = "vector")

test |> 
  ggplot(aes(y = NetUse, x = yhat_pruned)) +
  geom_jitter(alpha = 0.1, color = "blue") +
  geom_abline(color = "black") +
  theme_bw() +
  labs(title = "Pruned Regression Tree Model",
       x = "Predicted",
       y = "Actual")

# mse_pct(test$NetUse, yhat_pruned)

```

This model has a mean squared error of `r round(mse_pct(test$NetUse, yhat_pruned),1)`% of the variance of the actual values in the test dataset, a slight improvement over the Lasso Regression Model.  The Pruned Regression Tree Model has the advantage of being interpretable and easy to understand.

## Random Forest

```{r random_forest}

suppressMessages(library(ranger))
try(forest <- ranger(NetUse ~ ., 
                     data = train,
                     num.trees = 1000,
                     mtry = 2,
                     importance = "impurity")
    )

forest

sort(forest$variable.importance, decreasing = TRUE)/sum(forest$variable.importance)

yhat_forest <- predict(forest, test, type = "response")$predictions

test |> 
  ggplot(aes(y = NetUse, x = yhat_forest)) +
  geom_jitter(alpha = 0.1, color = "blue") +
  geom_abline(color = "black") +
  theme_bw() +
  labs(title = "Random Forest Model",
       x = "Predicted",
       y = "Actual")

# mse_pct(test$NetUse, yhat_forest)

```

This model has a mean squared error of `r round(mse_pct(test$NetUse, yhat_forest),1)`% of the variance of the actual values in the test dataset, a slight improvement over the Pruned Regression Tree Model.  The Random Forest Model has the advantage of being less sensitive to overfitting than the Pruned Regression Tree Model.

## XGBoost

```{r xgboost}

suppressMessages(library(xgboost))

xgb_cv <- xgb.cv(data = train_X, 
                 label = train$NetUse, 
                 nrounds = 500, 
                 nfold = 10,
                 eta = 0.1,
                 early_stopping_rounds = 10,
                 objective = "reg:squarederror", 
                 print_every_n = 10)

xgb <- xgboost(data = train_X, 
               label = train$NetUse, 
               nrounds = xgb_cv$best_iteration, 
               eta = 0.1,
               objective = "reg:squarederror",
               print_every_n = 10)

yhat_xgb <- predict(xgb, newdata = test_X)

test |> 
  ggplot(aes(y = NetUse, x = yhat_pruned)) +
  geom_jitter(alpha = 0.1, color = "blue") +
  geom_abline(color = "black") +
  theme_bw() +
  labs(title = "XGBoost Model",
       x = "Predicted",
       y = "Actual")

# mse_pct(test$NetUse, yhat_xgb)

```

This model has a mean squared error of `r round(mse_pct(test$NetUse, yhat_xgb),1)`% of the variance of the actual values in the test dataset, a slight improvement over the Random Forest Model.  The XGBoost Model has the advantage of being less sensitive to overfitting than the Random Forest Model.

## Seasonal ETS

```{r ets}

fit_ets <- train_ts |> 
  model(ETS(NetUse ~ error("A") + trend("N") + season("A")))

fit_ets |> report()

yhat_ets <- fit_ets |> forecast(h = nrow(test_ts)) |> as_tibble() |> pull(.mean)

test_ts |> 
  ggplot(aes(y = NetUse, x = yhat_ets)) +
  geom_point(alpha = 0.1, color = "blue") +
  geom_abline(color = "black") +
  theme_bw() +
  labs(title = "Seasonal ETS Model",
       x = "Predicted",
       y = "Actual")

# mse_pct(test_ts$NetUse, yhat_ets)

```

This ETS Model has a mean squared error of `r round(mse_pct(test_ts$NetUse, yhat_ets),1)`% of the variance of the actual values in the test dataset, permornimg much worse that all of the models presented so far.

## ARIMAx

```{r arimax}

# fit arimax model with exogenous variables, let fabletools find best pdqs
# fit_arimax <- train_ts |> 
#   model(ARIMA(NetUse ~ xreg(sunUp + workDay + Tdry + RH)))
# 
# fit_arimax |> report()
# 

fit_arimax <- train_ts |> 
  model(ARIMA(NetUse ~ pdq(1,0,3) + PDQ(2,1,0) + xreg(sunUp + workDay + Tdry + RH)))

fit_arimax |> report()

newdata <- test_ts |> 
  select(dttm, hour, quarter, Tdry, RH, sunUp, workDay) |> 
  as_tsibble(index = dttm)

yhat_arimax <- fit_arimax |> forecast(newdata) |> as_tibble() |> pull(.mean)

test |> 
  ggplot(aes(y = NetUse, x = yhat_arimax)) +
  geom_point(alpha = 0.1, color = "blue") +
  geom_abline(color = "black") +
  theme_bw() +
  labs(title = "ARIMAx Model",
       x = "Predicted",
       y = "Actual")

# mse_pct(test$NetUse, yhat_arimax)

```

This ARIMAx Model has a mean squared error of `r round(mse_pct(test$NetUse, yhat_arimax),1)`% of the variance of the actual values in the test dataset, and is fighting for the wooden spoon along with the ETS Model.  The ARIMAx Model has the advantage of being able to model the autocorrelation in the data, which can be useful when there are time-dependent patterns in the data (which there are).  Non-stationarity is probably the reason for the poor performance of the ARIMAx Model.

## Summary of Results

```{r results}

results <- tibble(Model = c("Seasonal Average",
                            "OLS Regression", 
                            "Lasso Regression", 
                            "Pruned Regression Tree", 
                            "Random Forest", 
                            "XGBoost", 
                            "Lasso Regression with Interactions", 
                            "Ridge Regression",
                            "Seasonal ETS",
                            "ARIMAx"),
                   mse_pct = c(mse_pct(test$NetUse, yhat_avg),
                               mse_pct(test$NetUse, yhat_ols),
                               mse_pct(test$NetUse, yhat_lasso),
                               mse_pct(test$NetUse, yhat_pruned),
                               mse_pct(test$NetUse, yhat_forest),
                               mse_pct(test$NetUse, yhat_xgb),
                               mse_pct(test$NetUse, yhat_lasso_int),
                               mse_pct(test$NetUse, yhat_ridge),
                               mse_pct(test_ts$NetUse, yhat_ets),
                               mse_pct(test$NetUse, yhat_arimax))
                  ) |> 
  mutate(mse_pct = round(mse_pct, 2)) |> 
  arrange(mse_pct) |> 
  print()

```


